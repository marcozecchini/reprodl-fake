{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34cce497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c78257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5f629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaabc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ef703",
   "metadata": {},
   "source": [
    "## Step 1: loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d89490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('data/ESC-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037ae3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c41e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(datapath /  Path('meta/esc50.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e133b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target        category  esc10  src_file take\n",
       "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
       "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
       "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
       "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
       "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e4f56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = torchaudio.load(datapath / 'audio' / csv.iloc[0,0], normalize=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d936dd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 220500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35818736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366b2825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# useful as an hyperparameter to reduce training time\n",
    "torchaudio.transforms.Resample(orig_freq=sr, new_freq=8000)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16eaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48cdf737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # 128 mel frequencies, 1103 is the time/frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0614cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torchaudio.transforms.AmplitudeToDB()(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee1fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f702b7eed60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4e8d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# everytime you want to work with data in Pytorch we need to use Dataset object\n",
    "class ESC50Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, path: Path = Path('data/ESC-50'),\n",
    "                sample_rate: int = 8000,\n",
    "                folds = [1]):\n",
    "        \n",
    "        # Load CSV file & initialize all torchaudio.transforms\n",
    "        # Resample --> Melspectrogram --> AmplitudeToDB\n",
    "\n",
    "        self.path = path\n",
    "        self.csv = pd.read_csv(datapath /  Path('meta/esc50.csv'))\n",
    "        self.csv = self.csv[self.csv['fold'].isin(folds)]\n",
    "        \n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=44100, new_freq=sample_rate)\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sr)\n",
    "        self.db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "    #function to index with [] square brackets\n",
    "    def __getitem__(self, index):\n",
    "        # Returns (xb, yb) pair\n",
    "        row = self.csv.iloc[index]\n",
    "        wav, _ = torchaudio.load(self.path / 'audio' / row['filename'])\n",
    "        label = row['target']\n",
    "        xb = self.db(\n",
    "            self.melspec(\n",
    "                self.resample(wav)\n",
    "            )\n",
    "        )\n",
    "        return xb, label\n",
    "    \n",
    "    # function tells pytorch how many object you have in this object\n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c2b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ESC50Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66a8453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in train_data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e63e370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 201])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab688eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fc3bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ESC50Dataset(folds=[1])\n",
    "val_data = ESC50Dataset(folds=[2])\n",
    "test_data = ESC50Dataset(folds=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80cb77e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch size has influence on convergence but also on memory\n",
    "train_loader = \\\n",
    "    torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "977c3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size has influence on convergence but also on memory\n",
    "val_loader = \\\n",
    "    torch.utils.data.DataLoader(val_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44eed885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size has influence on convergence but also on memory\n",
    "test_loader = \\\n",
    "    torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce43987",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e31f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioNet(pl.LightningModule):\n",
    " \n",
    "    def __init__(self, n_classes = 50, base_filters = 32): # or even 16 as base filters without GPU\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, base_filters, 11, padding=5)\n",
    "        self.bn1 = nn.BatchNorm2d(base_filters)\n",
    "        self.conv2 = nn.Conv2d(base_filters, base_filters, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(base_filters, base_filters * 2, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(base_filters * 2)\n",
    "        self.conv4 = nn.Conv2d(base_filters * 2, base_filters * 4, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(base_filters * 4)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(base_filters * 4, n_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.fc1(x[:, :, 0, 0])\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fd57466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0) # always do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45bce298",
   "metadata": {},
   "outputs": [],
   "source": [
    "audionet = AudioNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5857cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fcdb4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 128, 201])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66379119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audionet(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7786af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "589bcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/envs/reprodl/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "   | Name  | Type        | Params\n",
      "---------------------------------------\n",
      "0  | conv1 | Conv2d      | 3.9 K \n",
      "1  | bn1   | BatchNorm2d | 64    \n",
      "2  | conv2 | Conv2d      | 9.2 K \n",
      "3  | bn2   | BatchNorm2d | 64    \n",
      "4  | pool1 | MaxPool2d   | 0     \n",
      "5  | conv3 | Conv2d      | 18.5 K\n",
      "6  | bn3   | BatchNorm2d | 128   \n",
      "7  | conv4 | Conv2d      | 73.9 K\n",
      "8  | bn4   | BatchNorm2d | 256   \n",
      "9  | pool2 | MaxPool2d   | 0     \n",
      "10 | fc1   | Linear      | 6.5 K \n",
      "---------------------------------------\n",
      "112 K     Trainable params\n",
      "0         Non-trainable params\n",
      "112 K     Total params\n",
      "0.450     Total estimated model params size (MB)\n",
      "/home/marco/anaconda3/envs/reprodl/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708b909267c64cf99c5d22afbf10398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(audionet, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41995d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(audionet, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
